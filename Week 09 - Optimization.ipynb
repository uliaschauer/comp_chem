{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples of numerical optimization algorithms\n",
    "Based in part on code by Yiren Lu (https://github.com/yrlu/non-convex)\n",
    "\"\"\"\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# change parameters here\n",
    "function_type = 0 #change this to 0 for biquadratic, 1 for Rosenbrock and 2 for Himmelblau function\n",
    "starting_pos = np.array([1.2, 1.2])   #change starting position\n",
    "\n",
    "# parameters that probably won't need changing\n",
    "accuracy = 1e-4\n",
    "max_iterations = 1000\n",
    "\n",
    "# code for function definition\n",
    "def function(x):\n",
    "    if function_type == 0:\n",
    "        return 100 * x[0]**2 + 200 * x[1]**2\n",
    "    elif function_type == 1:\n",
    "        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "    elif function_type == 2:\n",
    "        return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "    else:\n",
    "        print(\"Unknown function_type: {}\".format(function_type))\n",
    "    \n",
    "def gradient(x):\n",
    "    if function_type == 0:\n",
    "        return np.array([200 * x[0], 400 * x[1]])\n",
    "    elif function_type == 1:\n",
    "        return np.array([200*(x[1]-x[0]**2)*(-2*x[0]) + 2*(x[0]-1), 200*(x[1]-x[0]**2)])\n",
    "    elif function_type == 2:\n",
    "        return np.array([4 * x[0] * (x[0]**2 + x[1] - 11) + 2 * (x[0] + x[1]**2 - 7),\n",
    "                         2 * (x[0]**2 + x[1] - 11) + 4 * x[1] * (x[0] + x[1]**2 - 7)])\n",
    "    else:\n",
    "        print(\"Unknown function_type: {}\".format(function_type))\n",
    "\n",
    "def hessian(x):\n",
    "    if function_type == 0:\n",
    "        return np.array([[200, 0], [0, 400]])\n",
    "    elif function_type == 1:\n",
    "        return np.array([[1200*x[0]**2 - 400*x[1] + 2, -400*x[0]], [-400*x[0], 200]])\n",
    "    elif function_type == 2:\n",
    "        return np.array([[4 * (x[0]**2 + x[1] - 11) + 8 * x[0]**2 + 2, 4 * x[0] + 4 * x[1]], \n",
    "                         [4 * x[0] + 4 * x[1], 4 * (x[0] + x[1]**2 -  7) + 8 * x[1]**2 + 2]])    \n",
    "    else:\n",
    "        print(\"Unknown function_type: {}\".format(function_type))\n",
    "        \n",
    "# code for line search\n",
    "def wolfe(f, g, xk, alpha, pk):\n",
    "    c1 = 1e-4\n",
    "    return f(xk + alpha * pk) <= f(xk) + c1 * alpha * np.dot(g(xk), pk)\n",
    "\n",
    "def strong_wolfe(f, g, xk, alpha, pk, c2):\n",
    "    # typically, c2 = 0.9 when using Newton or quasi-Newton's method.\n",
    "    #            c2 = 0.1 when using non-linear conjugate gradient method.\n",
    "    return wolfe(f, g, xk, alpha, pk) and abs(np.dot(g(xk + alpha * pk), pk)) <= c2 * abs(np.dot(g(xk), pk))\n",
    "\n",
    "def step_length(f, g, xk, alpha, pk, c2):\n",
    "    return interpolation(f, g,\n",
    "                         lambda alpha: f(xk + alpha * pk),\n",
    "                         lambda alpha: np.dot(g(xk + alpha * pk), pk),\n",
    "                         alpha, c2,\n",
    "                         lambda f, g, alpha, c2: strong_wolfe(f, g, xk, alpha, pk, c2))\n",
    "\n",
    "def interpolation(f, g, f_alpha, g_alpha, alpha, c2, strong_wolfe_alpha, iters=20):\n",
    "  # referred implementation here:\n",
    "  # https://github.com/tamland/non-linear-optimization\n",
    "    l = 0.0\n",
    "    h = 1.0\n",
    "    for i in range(iters):\n",
    "        if strong_wolfe_alpha(f, g, alpha, c2):\n",
    "            return alpha\n",
    "\n",
    "        half = (l + h) / 2\n",
    "        alpha = - g_alpha(l) * (h**2) / (2 * (f_alpha(h) - f_alpha(l) - g_alpha(l) * h))\n",
    "        if alpha < l or alpha > h:\n",
    "            alpha = half\n",
    "        if g_alpha(alpha) > 0:\n",
    "            h = alpha\n",
    "        elif g_alpha(alpha) <= 0:\n",
    "            l = alpha\n",
    "    return alpha\n",
    "\n",
    "# code for optimization algorithms\n",
    "def steepest_descent(f, grad, x0, iterations, error):\n",
    "    xhist = [x0]\n",
    "    x = x0\n",
    "    x_old = x\n",
    "    c2 = 0.9\n",
    "    for i in range(iterations):\n",
    "        pk = -grad(x)\n",
    "        alpha = step_length(f, grad, x, 1.0, pk, c2)\n",
    "        x = x + alpha * pk\n",
    "        xhist.append(x)\n",
    "        \n",
    "        if np.linalg.norm(x - x_old) < error:\n",
    "            break\n",
    "        x_old = x\n",
    "        \n",
    "    return xhist, i\n",
    "\n",
    "def conjugate_gradient(f, g, x0, iterations, error):\n",
    "    xhist = [x0]\n",
    "    xk = x0\n",
    "    c2 = 0.1\n",
    "\n",
    "    fk = f(xk)\n",
    "    gk = g(xk)\n",
    "    pk = -gk\n",
    "\n",
    "    for i in range(iterations):\n",
    "        alpha = step_length(f, g, xk, 1.0, pk, c2)\n",
    "        xk1 = xk + alpha * pk\n",
    "        gk1 = g(xk1)\n",
    "        beta_k1 = np.dot(gk1, gk1) / np.dot(gk, gk)\n",
    "        pk1 = -gk1 + beta_k1 * pk\n",
    "        xhist.append(xk1)\n",
    "        \n",
    "        if np.linalg.norm(xk1 - xk) < error:\n",
    "            xk = xk1\n",
    "            break\n",
    "\n",
    "        xk = xk1\n",
    "        gk = gk1\n",
    "        pk = pk1\n",
    "\n",
    "    return xhist, i\n",
    "\n",
    "def newton(f, g, H, x0, iterations, error):\n",
    "    xhist = [x0]\n",
    "    x = x0\n",
    "    x_old = x\n",
    "    c2 = 0.9\n",
    "    for i in range(iterations):\n",
    "        pk = -np.linalg.solve(H(x), g(x))\n",
    "        alpha = step_length(f, g, x, 1.0, pk, c2)\n",
    "        x = x + alpha * pk\n",
    "        xhist.append(x)\n",
    "        \n",
    "        if np.linalg.norm(x - x_old) < error:\n",
    "            break\n",
    "        x_old = x\n",
    "        \n",
    "    return xhist, i\n",
    "\n",
    "def bfgs(f, g, x0, iterations, error):\n",
    "    xhist = [x0]\n",
    "    xk = x0\n",
    "    c2 = 0.9\n",
    "    I = np.identity(xk.size)\n",
    "    Hk = I\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # compute search direction\n",
    "        gk = g(xk)\n",
    "        pk = -Hk.dot(gk)\n",
    "\n",
    "        # obtain step length by line search\n",
    "        alpha = step_length(f, g, xk, 1.0, pk, c2)\n",
    "\n",
    "        # update x\n",
    "        xk1 = xk + alpha * pk\n",
    "        gk1 = g(xk1)\n",
    "\n",
    "        # define sk and yk for convenience\n",
    "        sk = xk1 - xk\n",
    "        yk = gk1 - gk\n",
    "\n",
    "        # compute H_{k+1} by BFGS update\n",
    "        rho_k = float(1.0 / yk.dot(sk))\n",
    "\n",
    "        Hk1 = (I - rho_k * np.outer(sk, yk)).dot(Hk).dot(I - \\\n",
    "               rho_k * np.outer(yk, sk)) + rho_k * np.outer(sk, sk)\n",
    "        \n",
    "        if np.linalg.norm(xk1 - xk) < error:\n",
    "            xk = xk1\n",
    "            xhist.append(xk1)\n",
    "            break\n",
    "\n",
    "        Hk = Hk1\n",
    "        xk = xk1\n",
    "        xhist.append(xk1)\n",
    "\n",
    "    return xhist, i\n",
    "\n",
    "# plot function\n",
    "def npmap2d(fun, xs, ys):\n",
    "    Z = np.empty(len(xs) * len(ys))\n",
    "    i = 0\n",
    "    for y in ys:\n",
    "        for x in xs:\n",
    "            Z[i] = fun(np.array([x, y]))\n",
    "            i += 1\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z.shape = X.shape\n",
    "    return X, Y, Z\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.view_init(azim=0, elev=90)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "if function_type == 0:\n",
    "    step = 0.1\n",
    "    X = np.arange(-2., 2.+step, step)\n",
    "    Y = np.arange(-2., 2.+step, step)\n",
    "    zmin = 0\n",
    "    zmax = 600\n",
    "elif function_type == 1:\n",
    "    step = 0.1\n",
    "    X = np.arange(-2.5, 2.5+step, step)\n",
    "    Y = np.arange(-2., 4.+step, step)\n",
    "elif function_type == 2:\n",
    "    step = 0.2\n",
    "    X = np.arange(-6., 6.+step, step)\n",
    "    Y = np.arange(-6., 6.+step, step)\n",
    "    zmin = 0\n",
    "    zmax = 200\n",
    "else:\n",
    "    print(\"Unknown function type: {}\".format(function_type))\n",
    "X, Y, Z = npmap2d(function, X, Y)\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, vmin = zmin, vmax = zmax, linewidth=0, antialiased=True, alpha=0.5)\n",
    "fig.colorbar(surf)\n",
    "\n",
    "start = time.time()\n",
    "x, n_iter = steepest_descent(function, gradient, starting_pos, iterations=max_iterations, error=accuracy)\n",
    "end = time.time()\n",
    "print(\"Steepest Descent  \\n    {:4d} iterations, minimum = ({:.2f}, {:.2f}), f={:.2f}, time elapsed {:.4f} sec, secs per iteration {:.6f}\".format(n_iter, x[-1][0], x[-1][1], function(x[-1]), end - start, (end - start) / n_iter))\n",
    "ax.scatter([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='r', label='Steepest Descent')\n",
    "ax.plot3D([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='r')\n",
    "\n",
    "start = time.time()\n",
    "x, n_iter = conjugate_gradient(function, gradient, starting_pos, iterations=max_iterations, error=accuracy)\n",
    "end = time.time()\n",
    "print(\"Conjugate Gradient\\n    {:4d} iterations, minimum = ({:.2f}, {:.2f}), f={:.2f}, time elapsed {:.4f} sec, secs per iteration {:.6f}\".format(n_iter, x[-1][0], x[-1][1], function(x[-1]), end - start, (end - start) / n_iter))\n",
    "ax.scatter([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='g', label='Conjugate Gradient')\n",
    "ax.plot3D([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='g')\n",
    "\n",
    "start = time.time()\n",
    "x, n_iter = newton(function, gradient, hessian, starting_pos, iterations=max_iterations, error=accuracy)\n",
    "end = time.time()\n",
    "print(\"Newton            \\n    {:4d} iterations, minimum = ({:.2f}, {:.2f}), f={:.2f}, time elapsed {:.4f} sec, secs per iteration {:.6f}\".format(n_iter, x[-1][0], x[-1][1], function(x[-1]), end - start, (end - start) / n_iter))\n",
    "ax.scatter([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='b', label='Newton')\n",
    "ax.plot3D([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='b')\n",
    "\n",
    "start = time.time()\n",
    "x, n_iter = bfgs(function, gradient, starting_pos, iterations=max_iterations, error=accuracy)\n",
    "end = time.time()\n",
    "print(\"BFGS              \\n    {:4d} iterations, minimum = ({:.2f}, {:.2f}), f={:.2f}, time elapsed {:.4f} sec, secs per iteration {:.6f}\".format(n_iter, x[-1][0], x[-1][1], function(x[-1]), end - start, (end - start) / n_iter))\n",
    "ax.scatter([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='purple', label='BFGS')\n",
    "ax.plot3D([t[0] for t in x], [t[1] for t in x], [np.max(Z)]*len(x), c='purple')\n",
    "\n",
    "l = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
